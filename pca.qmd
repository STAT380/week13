---
title: "Untitled"
format: html
editor: visual
---


```{r}
packages<- c(
"dplyr",
"readr",
"tidyr",
"purrr",
"broom",
"magrittr",
"corrplot",
"caret",
"rpart",
"rpart.plot",
"e1071",
"torch",
"luz",
"ramify",
"keras"
)
sapply(packages, require, character.only=T)
```

```{r}
data<- tibble(
x1 = rnorm( 100 , mean = 0 ,sd = 1 ),
x2 = x1 +rnorm( 100 ,mean = 0 , sd =0.1),
x3 = x1 +rnorm( 100 ,mean = 0 , sd =0.1)
)
head(data) %>% knitr::kable()

```

```{r}
pca <- princomp(data, cor =TRUE)
summary(pca)
screeplot(pca, type="l")
```
one way to interpret the principal components is to examine the loadings, which are the weights that indicate the contribution of each original variable to the creation of each principal component. A high loading value indicates that a variable has a strong influence on that principal component. As an extra fact, we understood that the principal components can also be used for data reduction by selecting only the top PCs that capture the majority of the variability in the data. This can simplify the analysis and reduce the dimensionality of the data, while still retaining the most important information.

```{r}
set.seed( 42 )
n<- 500
science<- rnorm(n, mean = 60 ,sd = 10 )
humanities <- rnorm(n, mean = 80 ,sd= 10 )
df <- tibble(
math = 0.8 * science+ rnorm(n, mean = 0 , sd = 7 ),
physics =1.0 * science +rnorm(n, mean = 0 , sd = 5 ),
chemistry = 1.3 *science +rnorm(n, mean = 0 ,sd = 3 ),
history =0.8 * humanities +rnorm(n, mean = 0 ,sd = 5 ),
geography = 1.0 *humanities + rnorm(n, mean = 0 ,sd = 10 ),
literature = 1.2* humanities +rnorm(n, mean = 0 ,sd = 2 )
)
df$gpa <- (0.9 *science +0.5 * humanities +rnorm(n, mean= 0 ,sd= 10 )) * 4 / 100
df %>%
head() %>%
round(digits= 2 )%>%
knitr::kable()
```
```{r}
pca <- princomp(df%>% select(-gpa), cor=TRUE)

pca$loadings
plot(pca, type="l")
Z<- predict(pca, df)
```

$loadings: each row represent the original variables and the columns represent the principal components. Each element of the matrix represents the loading of the corresponding variable on the corresponding PC.



# PCregression

it is a regression technique that combines Principal Component Analysis (PCA) and multiple linear regression. The way it works is that the original predictor variables are transformed into a smaller set of uncorrelated principal components (PCs) using PCA. The PCs are then used as the predictors in a multiple linear regression model, instead of the original variables. By reducing the dimensionality of the predictor space, PCR can reduce the problem of multicollinearity and improve the stability of the regression estimates.

use a historgram to represent the variance for each PC.
```{r}
pca <- princomp(df %>% select(-gpa), cor=TRUE)
screeplot(pca)
```


Create a new data frame Z that contains the transformed (predicted) data in the principal component space

```{r}
Z <- predict(pca, df)
df_pca <- Z%>%
as_tibble%>%
select(Comp.1, Comp.2)%>%
mutate(gpa =df$gpa)

head(df_pca) %>% knitr::kable()
```

The resulting table shows the values of the first two principal components (Comp.1 and Comp.2) along with the corresponding GPA values. This table can be used to visualize the relationship between GPA and the first two principal components.

