---
title: "Week 12"
title-block-banner: true
title-block-style: default
execute:
  freeze: true
  cache: true
format: ipynb
# format: pdf
---

```{r}
#| tags: []
#| vscode: {languageId: r}
# renv::activate()
set.seed(380)
```


#### Packages we will require this week

```{r warnings=F, message=F, results='hide'}
#| tags: []
#| vscode: {languageId: r}
packages <- c(
    # Old packages
    "ISLR2",
    "dplyr",
    "tidyr",
    "readr",
    "purrr",
    "repr",
    "tidyverse",
    "kableExtra",
    "IRdisplay",
    # NEW
    "torch",
    "torchvision",
    "luz"
)

# renv::install(packages)
sapply(packages, require, character.only=TRUE)
```

---

### Agenda:

1. Real-world neural network classification
1. Dataloaders
1. Torch for image classification

<br><br><br>

## Titanic

```{r}
#| tags: []
#| vscode: {languageId: r}
url <- "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"

df <- read_csv(url) %>%
    mutate_if(\(x) is.character(x), as.factor) %>%
    mutate(y = Survived) %>%
    select(-c(Name, Survived)) %>%
    (\(x) {
        names(x) <- tolower(names(x))
        x
    })

df %>% head
```

## Breast Cancer Prediction

```{r}
#| tags: []
#| vscode: {languageId: r}
# url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"

# col_names <- c("id", "diagnosis", paste0("feat", 1:30))

# df <- read_csv(
#         url, col_names, col_types = cols()
#     ) %>% 
#     select(-id) %>% 
#     mutate(y = ifelse(diagnosis == "M", 1, 0)) %>%
#     select(-diagnosis)


# df %>% head
```

### Train/Test Split

```{r}
#| tags: []
#| vscode: {languageId: r}
k <- 5

test_ind <- sample(
    1:nrow(df), 
    floor(nrow(df) / k),
    replace=FALSE
)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
df_train <- df[-test_ind, ]
df_test  <- df[test_ind, ]

nrow(df_train) + nrow(df_test) == nrow(df)
```

### Benchmark with Logistic Regression

```{r}
#| tags: []
#| vscode: {languageId: r}
fit_glm <- glm(
    y ~ ., 
    df_train %>% mutate_at("y", factor), 
    family = binomial()
)

glm_test <- predict(
    fit_glm, 
    df_test,
    output = "response"
)

glm_preds <- ifelse(glm_test > 0.5, 1, 0)
table(glm_preds, df_test$y)
```

### Neural Net Model

```{r}
#| tags: []
#| vscode: {languageId: r}
NNet <- nn_module(
  initialize = function(p, q1, q2, q3) {  
    self$hidden1 <- nn_linear(p, q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2, q3)
    self$output <- nn_linear(q3, 1)
    self$activation <- nn_relu()
    self$sigmoid <- nn_sigmoid()
  },
    
  forward = function(x) {
    x %>% 
      self$hidden1() %>% self$activation() %>% 
      self$hidden2() %>% self$activation() %>% 
      self$hidden3() %>% self$activation() %>% 
      self$output() %>% self$sigmoid()
  }
)
```

### Fit using Luz
If a model has 0 intercept then when the intercept is 0 then everything else is also 0. This can be useful
at times. For example if modeling horsepower to car price then it makes sense that if the horsepower is
0 then so should the price

```{r}
#| tags: []
#| vscode: {languageId: r}
M <- model.matrix(y ~ 0 + ., data = df_train)
list(
    model.matrix(y ~ 0 + ., data = df_train),
    df_train %>% select(y) %>% as.matrix
    )
```

```{r}
#| tags: []
#| vscode: {languageId: r}
fit_nn <- NNet %>%
    #
    # Setup the model
    #
    setup(
        loss = nn_bce_loss(),
        optimizer = optim_adam, 
        metrics = list(
            luz_metric_accuracy()
        )
    ) %>% 
    #
    # Set the hyperparameters
    #
    set_hparams(p=ncol(M), q1=256, q2=128, q3=64) %>% 
    set_opt_hparams(lr=0.005) %>% 
    #
    # Fit the model
    #
    fit(
        data = list(
            model.matrix(y ~ 0 + ., data = df_train),
            df_train %>% select(y) %>% as.matrix
        ),
        valid_data = list(
            model.matrix(y ~ 0 + ., data = df_test),
            df_test %>% select(y) %>% as.matrix
        ),
        epochs = 50, 
        verbose = TRUE
    )
```

```{r}
#| tags: []
#| vscode: {languageId: r}
plot(fit_nn)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
nn_test <- predict(
    fit_nn, 
    model.matrix(y ~ . - 1, data = df_test)
)
# nn_test
nn_preds <- ifelse(nn_test > 0.5, 1, 0)

table(nn_preds, df_test$y)
mean(nn_preds == df_test$y)
```

```{r}
#| vscode: {languageId: r}
table(glm_preds, df_test$y)
mean(glm_preds == df_test$y)
```
In this case the neural net did worse than logistic regression but the good part is that we can change
some parameters like the learning rate that will give us better results. Also these were small data sets
but if we were to do this with bigger ones then the neural nets would outperform the logistic regression
models in a more noticable way.

<br><br><br><br>

---

### DataLoaders

* Dataloaders are a key component in the machine learning pipeline.

* They handle loading and preprocessing data in a way that is efficient for training and evaluating models.

* Dataloaders make it easy to work with large datasets by loading the data in smaller chunks (called **batches**) and applying transformations _on-the-fly_.


In the context of machine learning, a DataLoader is a utility used for efficiently loading and processing data during the training or evaluation of models, particularly in deep learning frameworks like PyTorch or TensorFlow.


1. **Batching**: DataLoaders typically load data in batches. Instead of loading the entire dataset at once, which may not fit into memory for large datasets, DataLoader loads a small subset (batch) of the data at a time. Batching is crucial for training deep learning models efficiently, as it allows the model to update its parameters based on multiple samples at once, which often leads to more stable training and faster convergence.

2. **Shuffling**: When training machine learning models, it's common practice to shuffle the dataset before each epoch (complete pass through the dataset). Shuffling ensures that the model doesn't learn from the order of the data and helps prevent any bias that might arise from the order of the samples. DataLoader typically provides an option to shuffle the data while loading batches.

3. **Parallelism**: Many modern deep learning frameworks support parallel processing, which can significantly speed up data loading. DataLoader often leverages this parallelism by loading batches in parallel, especially when the data is stored on disk or in remote storage systems.

4. **Data Transformation**: DataLoader often includes functionality for applying transformations to the data on-the-fly. These transformations can include data augmentation (e.g., random rotations, flips, crops for image data), normalization (scaling pixel values, z-score normalization), or any other preprocessing steps required for the model.

5. **Integration with Frameworks**: DataLoader is typically integrated into deep learning frameworks such as PyTorch or TensorFlow, making it easy to use with other components of the framework like models, loss functions, and optimizers.
    

##### Why use Dataloaders?

> * **Efficient memory management:** loading data in smaller chunks reduces memory usage.
>
> * **Parallelism:**  supports asynchronous data loading for faster processing.
>
> * **Preprocessing:**  apply data transformations on-the-fly during training and evaluation.
>
> * **Flexibility:**  easily switch between different datasets or preprocessing steps.
>
> * **Standardization:**  consistent data format across various machine learning projects.
>

```{r}
#| tags: []
#| vscode: {languageId: r}
# ?dataloader
```

the transform function takes an input, converts it into a PyTorch tensor, flattens the tensor, and then normalizes its values by dividing each element by 255. This is often used as a preprocessing step for image data before feeding it into a neural network

```{r}
#| tags: []
#| vscode: {languageId: r}
transform <- function(x) x %>% 
    torch_tensor() %>% 
    torch_flatten() %>% 
    torch_div(255)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
dir <- "./mnist"

train_ds <- torchvision::mnist_dataset(
    root = dir,
    train = TRUE,
    download = TRUE,
    transform = transform
)
test_ds <- torchvision::mnist_dataset(
    root = dir,
    train = FALSE,
    download = TRUE,
    transform = transform
)
```


In R, an "environment" is a data structure that stores a collection of symbol-value pairs, similar to a dictionary or a hash table in other programming languages. Each symbol (or variable name) in R is associated with a value, and these associations are stored within an environment.

```{r}
#| tags: []
#| vscode: {languageId: r}
typeof(train_ds)
length(train_ds)
dim(train_ds$data)
train_ds$data[42000, ,]
```

```{r}
#| tags: []
#| vscode: {languageId: r}
options(repr.plot.width=10, repr.plot.height=10)

i <- sample(1:length(train_ds), 1)
x <- train_ds$data[i, ,] %>% t

image(x[1:28, 28:1], useRaster=TRUE, axes=FALSE, col=gray.colors(1000), main = train_ds$targets[i]-1 )
```

```{r}
#| tags: []
#| vscode: {languageId: r}
options(repr.plot.width = 10, repr.plot.height = 10)
par(mfrow=c(3,3))

for(iter in 1:9){
    i <- sample(1:length(train_ds), 1)
    x <- train_ds$data[i, ,] %>% t
    image(x[1:28, 28:1], useRaster = TRUE, axes = FALSE, col = gray.colors(1000), main = train_ds$targets[i]-1)
}
```

<br><br><br><br>
<br><br><br><br>

---

# Image Classification

```{r}
#| tags: []
#| vscode: {languageId: r}
train_dl <- dataloader(train_ds, batch_size = 1024, shuffle = TRUE)
test_dl <- dataloader(test_ds, batch_size = 1024)
typeof(train_dl)
train_dl
```

```{r}
#| tags: []
#| vscode: {languageId: r}
NNet_10 <- nn_module(
  initialize = function(p, q1, q2, q3, o) {
    self$hidden1 <- nn_linear(p, q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2, q3)
    self$OUTPUT <- nn_linear(q3, o)
    self$activation <- nn_relu()
  },
  forward = function(x) {
    x %>%
      self$hidden1() %>%
      self$activation() %>%
      self$hidden2() %>%
      self$activation() %>%
      self$hidden3() %>%
      self$activation() %>%
      self$OUTPUT()
  }
)
```

```{r}
#| tags: []
#| vscode: {languageId: r}
fit_nn <- NNet_10 %>%
    #
    # Setup the model
    #
    setup(
        loss = nn_cross_entropy_loss(),
        optimizer = optim_adam,
        metrics = list(
            luz_metric_accuracy()
        )
    ) %>%
    #
    # Set the hyperparameters
    #
    set_hparams(p=28*28, q1=256, q2=128, q3=64, o=10) %>% 
    #
    # Fit the model
    #
    fit(
        epochs = 3,
        data = train_dl,
        # valid_data = test_dl,
        verbose=TRUE
    )
```
```{r}
NN10_preds <- fit_nn %>% 
  predict(test_ds) %>% 
  torch_argmax(dim = 2) %>% 
  as_array()

# accuracy
mean(NN10_preds == test_ds$targets)
```
```{r}
# confusion matrix
table(NN10_preds, test_ds$targets)
```
```{r}
caret::confusionMatrix(NN10_preds %>% as.factor,
                       test_ds$targets %>% as.factor)
```
